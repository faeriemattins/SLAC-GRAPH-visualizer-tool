{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae727c1",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "from utils import load_sessions, read_session\n",
    "from main import generate_buffer\n",
    "from events import generate_event_seq\n",
    "from summary import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6eb411",
   "metadata": {},
   "source": [
    "# Compute summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = load_sessions()\n",
    "# sessions = load_sessions()[:10]\n",
    "\n",
    "file_name = []\n",
    "text = []\n",
    "sentence_metrics_list = []\n",
    "api_metrics_list = []\n",
    "\n",
    "err = []\n",
    "\n",
    "for sess in tqdm(sessions):\n",
    "    events = read_session(sess, verbose=0)\n",
    "    try:\n",
    "        text_buffer = generate_buffer(events)\n",
    "    except:\n",
    "        err.append(str(sess.split('/')[-1]) + \" is throwing an error!\")\n",
    "        continue\n",
    "    file_name.append(sess.split('/')[-1])\n",
    "    text.append(text_buffer[-1])\n",
    "    event_seq_dict = generate_event_seq(buffer=text_buffer,\n",
    "                                        events=events)\n",
    "    sentence_metrics, api_metrics = stats(event_seq_dict)\n",
    "    sentence_metrics_list.append(sentence_metrics)\n",
    "    api_metrics_list.append(api_metrics)\n",
    "    \n",
    "for e in err:\n",
    "    print(e)\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"file_name\"] = file_name\n",
    "df[\"text\"] = text\n",
    "\n",
    "for col in sentence_metrics_list[0]:\n",
    "    df[str(col)] = [x[col] for x in sentence_metrics_list]\n",
    "    \n",
    "for col in api_metrics_list[0]:\n",
    "    df[str(col)] = [x[col] for x in api_metrics_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953689d0",
   "metadata": {},
   "source": [
    "# Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c789b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(num1, num2):\n",
    "    return float(num1 / num2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603811d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3 : Total Sentences\n",
    "\n",
    "df[\"GPT-3 : Total Sentences\"] = list(map(get_ratio, \n",
    "    df[\"Number of sentences completely authored by GPT-3\"], \n",
    "    df[\"Total number of sentences\"]\n",
    "))\n",
    "\n",
    "df[\"GPT-3 : Total Sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac8020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User : Total Sentences\n",
    "\n",
    "df[\"User : Total Sentences\"] = list(map(get_ratio, \n",
    "    df[\"Number of sentences completely authored by the user\"], \n",
    "    df[\"Total number of sentences\"]\n",
    "))\n",
    "\n",
    "df[\"User : Total Sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type Token Ratio\n",
    "\n",
    "def get_ttr(text):\n",
    "    sentence_tokens = word_tokenize(text)\n",
    "    punctuations = list(string.punctuation)\n",
    "    sentence_tokens_clean = [word for word in sentence_tokens if word not in punctuations]\n",
    "    ttr = len(set(sentence_tokens_clean)) / len(sentence_tokens_clean)\n",
    "    return ttr\n",
    "\n",
    "\n",
    "df[\"Type Token Ratio\"] = df[\"text\"].apply(get_ttr)\n",
    "df[\"Type Token Ratio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa9124",
   "metadata": {},
   "source": [
    "# Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910663e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Sentence Metrics\")\n",
    "for col in sentence_metrics_list[0]:\n",
    "    print(\"Mean of\", col, \":\", np.mean(df[col]))\n",
    "    \n",
    "print(\"\\nAPI Metrics\")\n",
    "for col in api_metrics_list[0]:\n",
    "    print(\"Mean of\", col, \":\", np.mean(df[col]))\n",
    "    \n",
    "print(\"\\nRatios\")\n",
    "print(\"Mean of GPT-3 / Total Sentences : \", np.mean(df[\"GPT-3 : Total Sentences\"]))\n",
    "print(\"Mean of User / Total Sentences : \", np.mean(df[\"User : Total Sentences\"]))\n",
    "print(\"Mean of Type Token Ratio : \", np.mean(df[\"Type Token Ratio\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e141fa18",
   "metadata": {},
   "source": [
    "# Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca15835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Sentence Metrics\")\n",
    "for col in sentence_metrics_list[0]:\n",
    "    print(\"Median of\", col, \":\", np.median(df[col]))\n",
    "    \n",
    "print(\"\\nAPI Metrics\")\n",
    "for col in api_metrics_list[0]:\n",
    "    print(\"Median of\", col, \":\", np.median(df[col]))\n",
    "\n",
    "print(\"\\nRatios\")\n",
    "print(\"Median of GPT-3 / Total Sentences : \", np.median(df[\"GPT-3 : Total Sentences\"]))\n",
    "print(\"Median of User / Total Sentences : \", np.median(df[\"User : Total Sentences\"]))\n",
    "print(\"Median of Type Token Ratio : \", np.median(df[\"Type Token Ratio\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e120ed",
   "metadata": {},
   "source": [
    "# Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Sentence Metrics\")\n",
    "for col in sentence_metrics_list[0]:\n",
    "    print(\"Standard Deviation of\", col, \":\", np.std(df[col]))\n",
    "    \n",
    "print(\"\\nAPI Metrics\")\n",
    "for col in api_metrics_list[0]:\n",
    "    print(\"Standard Deviation of\", col, \":\", np.std(df[col]))\n",
    "    \n",
    "print(\"\\nRatios\")\n",
    "print(\"Standard Deviation of GPT-3 / Total Sentences : \", np.std(df[\"GPT-3 : Total Sentences\"]))\n",
    "print(\"Standard Deviation of User / Total Sentences : \", np.std(df[\"User : Total Sentences\"]))\n",
    "print(\"Standard Deviation of Type Token Ratio : \", np.std(df[\"Type Token Ratio\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a5b7a",
   "metadata": {},
   "source": [
    "# Minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3320f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Sentence Metrics\")\n",
    "for col in sentence_metrics_list[0]:\n",
    "    print(\"Minimum of\", col, \":\", np.min(df[col]))\n",
    "    \n",
    "print(\"\\nAPI Metrics\")\n",
    "for col in api_metrics_list[0]:\n",
    "    print(\"Minimum of\", col, \":\", np.min(df[col]))\n",
    "    \n",
    "print(\"\\nRatios\")\n",
    "print(\"Minimum of GPT-3 / Total Sentences : \", np.min(df[\"GPT-3 : Total Sentences\"]))\n",
    "print(\"Minimum of User / Total Sentences : \", np.min(df[\"User : Total Sentences\"]))\n",
    "print(\"Minimum of Type Token Ratio : \", np.min(df[\"Type Token Ratio\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cfa28f",
   "metadata": {},
   "source": [
    "# Maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Sentence Metrics\")\n",
    "for col in sentence_metrics_list[0]:\n",
    "    print(\"Maximum of\", col, \":\", np.max(df[col]))\n",
    "    \n",
    "print(\"\\nAPI Metrics\")\n",
    "for col in api_metrics_list[0]:\n",
    "    print(\"Maximum of\", col, \":\", np.max(df[col]))\n",
    "    \n",
    "print(\"\\nRatios\")\n",
    "print(\"Maximum of GPT-3 / Total Sentences : \", np.max(df[\"GPT-3 : Total Sentences\"]))\n",
    "print(\"Maximum of User / Total Sentences : \", np.max(df[\"User : Total Sentences\"]))\n",
    "print(\"Maximum of Type Token Ratio : \", np.max(df[\"Type Token Ratio\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7c5c5",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d391f85",
   "metadata": {},
   "source": [
    "# Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c47157",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"writing_session_stats.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
